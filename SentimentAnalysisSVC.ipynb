{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f289435-db2e-4de0-856a-bd0565661c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "training_data_path = 'training.csv'\n",
    "validation_data_path = 'validation.csv'\n",
    "test_data_path = 'test.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "training_data = pd.read_csv(training_data_path)\n",
    "validation_data = pd.read_csv(validation_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Display the first few rows of each dataset to understand the structure\n",
    "(training_data.head(), validation_data.head(), test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581d20c-08c3-48b2-a4ba-144124e2b8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4cefc1-ff75-4d87-8684-d32fa2aa2381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec525c-4d26-4291-9f41-2adffd04482b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbddb4d-11a3-4a10-875c-7b3985cce0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8f02d-7610-443c-9eb8-2421884d1c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = training_data.drop_duplicates()\n",
    "validation_data = validation_data.drop_duplicates()\n",
    "test_data = test_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5ea7d-0e67-41fe-9d98-f0b6e09eb264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the preprocessing function including stop words removal\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatization and handling negations\n",
    "    prev_word = \"\"\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        if word in ENGLISH_STOP_WORDS:\n",
    "            continue\n",
    "        if word == \"not\":\n",
    "            prev_word = \"not_\"\n",
    "        else:\n",
    "            if prev_word == \"not_\":\n",
    "                word = prev_word + word\n",
    "                prev_word = \"\"\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            # Remove punctuation and numbers\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            word = re.sub(r'\\d+', '', word)\n",
    "            processed_tokens.append(word)\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Load the datasets again\n",
    "training_data = pd.read_csv('training.csv')\n",
    "validation_data = pd.read_csv('validation.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Apply the preprocessing to the text data\n",
    "training_data['text'] = training_data['text'].apply(preprocess_text)\n",
    "validation_data['text'] = validation_data['text'].apply(preprocess_text)\n",
    "test_data['text'] = test_data['text'].apply(preprocess_text)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer without max_features to keep all words\n",
    "# Configure the TF-IDF vectorizer to include bi-grams and tri-grams and to ignore rare words that appear in less than two documents.\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3), min_df=2)\n",
    "\n",
    "# Fit the vectorizer on the training text data and transform all datasets\n",
    "tfidf_vectorizer.fit(training_data['text'])\n",
    "training_data_tfidf = tfidf_vectorizer.transform(training_data['text'])\n",
    "validation_data_tfidf = tfidf_vectorizer.transform(validation_data['text'])\n",
    "test_data_tfidf = tfidf_vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Save the TF-IDF data as .npz files since they are in sparse format\n",
    "preprocessed_data_dir = 'Preprocessed Data/'\n",
    "os.makedirs(preprocessed_data_dir, exist_ok=True)\n",
    "\n",
    "# Define file paths for the TF-IDF data\n",
    "training_data_tfidf_file = os.path.join(preprocessed_data_dir, 'training_tfidf.npz')\n",
    "validation_data_tfidf_file = os.path.join(preprocessed_data_dir, 'validation_tfidf.npz')\n",
    "test_data_tfidf_file = os.path.join(preprocessed_data_dir, 'test_tfidf.npz')\n",
    "\n",
    "# Save the TF-IDF data\n",
    "sp.save_npz(training_data_tfidf_file, training_data_tfidf)\n",
    "sp.save_npz(validation_data_tfidf_file, validation_data_tfidf)\n",
    "sp.save_npz(test_data_tfidf_file, test_data_tfidf)\n",
    "\n",
    "# Return the file paths for confirmation\n",
    "(training_data_tfidf_file, validation_data_tfidf_file, test_data_tfidf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0428f7-2687-40ee-ad0f-4142604886b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SVC Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6984c8-bbf6-48e6-8788-af415b0da5b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# File paths for TF-IDF data\n",
    "training_data_tfidf_file = 'training_tfidf.npz'\n",
    "validation_data_tfidf_file = 'validation_tfidf.npz'\n",
    "test_data_tfidf_file = 'test_tfidf.npz'\n",
    "\n",
    "# Load the original data with labels\n",
    "original_training_data = pd.read_csv('training.csv')  \n",
    "original_validation_data = pd.read_csv('validation.csv') \n",
    "original_test_data = pd.read_csv('test.csv')  \n",
    "\n",
    "# Extract labels from the original data\n",
    "training_labels = original_training_data['label']\n",
    "validation_labels = original_validation_data['label']\n",
    "test_labels = original_test_data['label']\n",
    "\n",
    "# Load the TF-IDF data\n",
    "training_data_tfidf = sp.load_npz(training_data_tfidf_file)\n",
    "validation_data_tfidf = sp.load_npz(validation_data_tfidf_file)\n",
    "test_data_tfidf = sp.load_npz(test_data_tfidf_file)\n",
    "\n",
    "# Initialize the Support Vector classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier\n",
    "svc.fit(training_data_tfidf, training_labels)\n",
    "\n",
    "# Predict on validation and test data\n",
    "validation_predictions = svc.predict(validation_data_tfidf)\n",
    "test_predictions = svc.predict(test_data_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(validation_labels, validation_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(validation_labels, validation_predictions))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(validation_labels, validation_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0219c45-c68e-4199-88ca-ba744d8967aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SVC Hyperparameter Tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064660d-5179-43ef-a946-5d7671da6641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Load your dataset\n",
    "original_training_data = pd.read_csv('training.csv')  \n",
    "original_validation_data = pd.read_csv('validation.csv') \n",
    "original_test_data = pd.read_csv('test.csv')  \n",
    "\n",
    "# Assuming your dataset has 'text' column for the input text and 'label' column for the labels\n",
    "text_data = original_training_data['text']\n",
    "labels = original_training_data['label']\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(test_texts, test_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "train_tfidf = vectorizer.fit_transform(train_texts)\n",
    "val_tfidf = vectorizer.transform(val_texts)\n",
    "test_tfidf = vectorizer.transform(test_texts)\n",
    "\n",
    "# Convert TF-IDF matrices to pandas DataFrames\n",
    "train_tfidf_df = pd.DataFrame(train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "val_tfidf_df = pd.DataFrame(val_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "test_tfidf_df = pd.DataFrame(test_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the resulting DataFrames\n",
    "print(\"Train TF-IDF DataFrame\")\n",
    "display(train_tfidf_df)\n",
    "\n",
    "print(\"\\nValidation TF-IDF DataFrame\")\n",
    "display(val_tfidf_df)\n",
    "\n",
    "# Display the Test TF-IDF DataFrame\n",
    "print(\"\\nTest TF-IDF DataFrame\")\n",
    "display(test_tfidf_df)\n",
    "\n",
    "# Initialize the Support Vector classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'C' : [1, 10, 100],\n",
    "    'kernel' : ['linear','rbf']\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Train the classifier with the best hyperparameters\n",
    "best_svc = SVC(C = best_params['C'], kernel=best_params['kernel'])\n",
    "best_svc.fit(train_tfidf, train_labels)\n",
    "\n",
    "# Predict on validation data\n",
    "val_predictions = best_svc.predict(val_tfidf)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "print(\"\\nValidation Set Performance\")\n",
    "print(classification_report(val_labels, val_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(val_labels, val_predictions))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_predictions))\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = best_svc.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "print(\"\\nTest Set Performance\")\n",
    "print(classification_report(test_labels, test_predictions))\n",
    "print(\"Accuracy:\", accuracy_score(test_labels, test_predictions))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
